\documentclass[11pt]{article}

% ------------------------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% ------------------------------------------------------------------------------

\usepackage[margin=.8in,top=1.1in,bottom=1.1in]{geometry} % page layout
\usepackage{amsmath,amsthm,amssymb,amsfonts} % math things
\usepackage{graphicx} % include graphics
\usepackage{fancyhdr} % header customization
\usepackage{titlesec} % help with section naming
\usepackage{listings}
\usepackage[final]{pdfpages}

% naming sections
\titleformat{\section}{\bf}{Problem \thesection}{0.5em}{}
\newcommand{\exercise}{\section{}}

% headers
\pagestyle{fancy}
\fancyhf{} % clear all
\fancyhead[L]{\sffamily\small Machine Learning 1 --- Homework}
\fancyhead[R]{\sffamily\small Page \thepage}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.2pt}
\markright{\hrulefill\quad}

\newcommand{\hwhead}[4]{
\begin{center}
\sffamily\large\bfseries Machine Learning Worksheet #1
\vspace{2mm}
\normalfont

#2\\
#3\\
\texttt{#4}
\end{center}
\vspace{6mm} \hrule \vspace{4mm}
}

% ------------------------------------------------------------------------------
% Start here -- Fill in your name, imat and email
% (and the same for who you worked with)
% You are allowed to work in groups of 2 (or 3 if there is no way around it)
% However, you each must submit individually - (it may be same file)
% ------------------------------------------------------------------------------

\newcommand{\names}{Tomas Ladek, Michael Kratzer} %
\newcommand{\imats}{3602673, 3612903} %
\newcommand{\emails}{tom.ladek@tum.de, mkratzer@mytum.de} %

\begin{document}

% ------------------------------------------------------------------------------
% Change xx (and only xx) to the current sheet number
% ------------------------------------------------------------------------------
\hwhead{11}{\names}{\imats}{\emails}

% ------------------------------------------------------------------------------
% Fill in your solutions
% ------------------------------------------------------------------------------

\exercise
Consider a Gaussian Mixture Model that describes the data points $\boldsymbol{x}$
\begin{align*}
	p (x | \theta) = \sum_{k = 0}^{K} \pi_k \mathcal N(x | \mu_k, \sigma^2 \boldsymbol{I})
\end{align*}
with some $\sigma \in \mathbb{R}$. Let us define $\pi_k = \dfrac{|C_k|}{N}$ with $|C_k|$ being the number of data point belonging to a cluster $k$ and $N$ the total number of data points.\\

\noindent Since we cannot easily optimize
\begin{align*}
	\arg \max_{\theta} \ln p(\mathcal{D}|\theta) = \sum_{n=1}^{N} \ln \sum_{k=0}^{K} \dfrac{|C_k|}{N} \mathcal{N}(x_n | \mu_k, \sigma^2 \boldsymbol{I})
\end{align*}
we use the EM algorithm. In EM, we are doing alternate coordinate ascent on $\mathcal{L}_{ELBO}(q, \theta)$ for some posterior distribution $q(z)$, i.e. two steps:
\begin{enumerate}
	\item Optimization w.r.t. $q$:
	\begin{align*}
		\arg \max_q \mathcal{L}_{ELBO}(q, \theta) &= \arg \max_q \mathbb{E}_{q(z)}[\ln p(x, z | \theta)] + \text{H}(q)
	\end{align*}
	With
	\begin{align*}
		\mathbb{E}_{q(z)}[\ln p(x, z | \theta)] &= \mathbb{E}_{q(z)}[\ln p(z | x, \theta)] + \mathbb{E}_{q(z)}[\ln p(x | \theta)]
	\end{align*}
	one arrives at
	\begin{align*}
		\arg \max_q \mathcal{L}_{ELBO}(q, \theta) &= \arg \max_q \big(\mathbb{E}_{q(z)}[\ln p(z | x, \theta)] + \mathbb{E}_{q(z)}[\ln p(x | \theta)] + \text{H}(q)\big)
	\end{align*}

	\item Optimization w.r.t. parameters $\theta$:
	\begin{align*}
	\arg \max_{\theta} \mathcal{L}_{ELBO}(q, \theta) &= \arg \max_{\theta} \big(\mathbb{E}_{q(z)}[\ln p(x, z | \theta)] + \text{H}(q)\big)\\
	&= \arg \max_{\theta} \big(\mathbb{E}_{q(z)}[\ln p(x, z | \theta)]\big)\\
	&= \arg \max_{\theta} \big(\mathbb{E}_{q(z)}[\ln p(z | x, \theta)] + \mathbb{E}_{q(z)}[\ln p(x | \theta)]\big)
	\end{align*}
\end{enumerate}

\noindent The \textit{K-Means algorithm} is an instance of the EM-algorithm. In the first step (E-Step), using
\begin{align*}
	p(z|x,\theta) &= r_{nk}(\theta) \sim \mathcal{N}(d(x_n, \mu_k) | \mu_k, \sigma^2 \boldsymbol{I})
	\quad\quad \text{with}\quad d(x_n, \mu_k) = ||x_n - \mu_k||_2
\end{align*}
and taking $\sigma$ towards $0$, one can see that $\mathcal{L}_{ELBO}(q, \theta)$ is maximized by picking a $j$ for every $x_n$ such that $d(x_n, \mu_j)$ is minimized, which corresponds to the step of ''calculating clusters'' in \textit{K-Means}. [Euclidean distance puts equidistant points on a circle (2D) resp. sphere (3D), so does a Gaussian with $\Sigma = \sigma^2 \boldsymbol{I}$]\\

\noindent In the second step (M-step) the optimization of the arguments $\theta$ is done, namely the $\mu_k$ are updated as follows:
\begin{align*}
	\mu_k \leftarrow \dfrac{1}{|C_k|} \sum_{x \in C_k} x
\end{align*}
and thus maximizing $\mathcal{L}_{ELBO}(q, \theta)$, which corresponds to the step ''recalibrating the cluster mean''.

\exercise
?

\exercise
The KL divergence for two Gaussian distributions of dimension $k$ is defined as follows:
\begin{align*}
	KL(\mathcal{N}_1 || \mathcal{N}_2) &= \dfrac{1}{2} \Big(\text{tr}(\Sigma_2^{-1}\Sigma_1) + (\mu_2 - \mu_1)^T \Sigma_2^{-1}(\mu_2 - \mu_1) - k + \ln\big(\dfrac{\det \Sigma_2}{\det \Sigma_1}\big)\Big)
\end{align*}
With $\Sigma_1$ and $\Sigma_2$ being diagonal, this amounts to
\begin{align*}
	KL(\mathcal{N}_1 || \mathcal{N}_2) &= \dfrac{1}{2} \Big(\sum_k \dfrac{\sigma_{1_k}}{\sigma_{2_k}} + (\mu_2 - \mu_1)^T \Sigma_2^{-1}(\mu_2 - \mu_1) - k + \ln\big(\prod_k \sigma_{2_k}{}\big) - \ln\big(\prod_k \sigma_{1_k}\big)\Big)
\end{align*}

\exercise
When leaving out the KL term from $\mathcal{L}_{ELBO}$ one runs the risk of overfitting through divergence. The KL term punishes dissimilarity of the approximate and the true posterior, i.e. with unlucky initialization, maximizing $\mathbb{E}_{q(z)}$ alone could lead to $q(z)$ that, while fitting the data well, doesn't have anything to do with the true distribution of the data.

\end{document}