\documentclass[11pt]{article}

% ------------------------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% ------------------------------------------------------------------------------

\usepackage[margin=.8in,top=1.1in,bottom=1.1in]{geometry} % page layout
\usepackage{amsmath,amsthm,amssymb,amsfonts} % math things
\usepackage{graphicx} % include graphics
\usepackage{fancyhdr} % header customization
\usepackage{titlesec} % help with section naming
\usepackage{listings}
\usepackage[final]{pdfpages}

% naming sections
\titleformat{\section}{\bf}{Problem \thesection}{0.5em}{}
\newcommand{\exercise}{\section{}}

% headers
\pagestyle{fancy}
\fancyhf{} % clear all
\fancyhead[L]{\sffamily\small Machine Learning 1 --- Homework}
\fancyhead[R]{\sffamily\small Page \thepage}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.2pt}
\markright{\hrulefill\quad}

\newcommand{\hwhead}[4]{
\begin{center}
\sffamily\large\bfseries Machine Learning Worksheet #1
\vspace{2mm}
\normalfont

#2\\
#3\\
\texttt{#4}
\end{center}
\vspace{6mm} \hrule \vspace{4mm}
}

% ------------------------------------------------------------------------------
% Start here -- Fill in your name, imat and email
% (and the same for who you worked with)
% You are allowed to work in groups of 2 (or 3 if there is no way around it)
% However, you each must submit individually - (it may be same file)
% ------------------------------------------------------------------------------

\newcommand{\names}{Tomas Ladek, Michael Kratzer} %
\newcommand{\imats}{3602673, 3612903} %
\newcommand{\emails}{tom.ladek@tum.de, mkratzer@mytum.de} %

\begin{document}

% ------------------------------------------------------------------------------
% Change xx (and only xx) to the current sheet number
% ------------------------------------------------------------------------------
\hwhead{5}{\names}{\imats}{\emails}

% ------------------------------------------------------------------------------
% Fill in your solutions
% ------------------------------------------------------------------------------

\exercise
\begin{align*}
	E_{\cal D}(w) &= \dfrac{1}{2} \sum_{n=1}^N T_n [\mathbf{W}^T \phi(x_n) - Z_n]^2\\
	&= \dfrac{1}{2} [T (\Phi W - Z)^T] [T (\Phi W - Z)]
\end{align*}
with
\begin{align*}
	T &= 
	\begin{pmatrix}
	\sqrt{T_1} & & 0 \\
	 & \ddots & \\
	 0 & & \sqrt{T_n}
	\end{pmatrix}
\end{align*}
Now, finding the optimal $W$ so that this error function is minimal (using the knowledge about the derivative from the slides and the Matrix Cookbook):
\begin{align*}
	\nabla_W E_{\cal D}(W) &= \dfrac{\partial}{\partial W} \dfrac{1}{2} [T (\Phi W - Z)^T] [T (\Phi W - Z)] \\
	&= - T^2 \Phi^T (\Phi W - Z)
\end{align*}
Further, setting this to $0$ and solving for $W$:
\begin{align*}
	- T^2 \Phi^T (\Phi W - Z) &= 0 \\
	- T^2 \Phi^T \Phi W + T^2 \Phi^T Z &= 0 \\
	T^2 \Phi^T \Phi W &= T^2 \Phi^T Z \\
	(T^2 \Phi^T \Phi)^{-1} (T^2 \Phi^T \Phi W) &= (T^2 \Phi^T \Phi)^{-1} T^2 \Phi^T Z \\
	W &= T^{-2} T^2 (\Phi^T \Phi)^{-1} \Phi^T Z \\
	W &= (\Phi^T \Phi)^{-1} \Phi^T Z	
\end{align*}

By carefully choosing values of $T_n$, the error function can be made less sensitive against outliers (that can have a significant impact on the variance of the data noise). Also duplicate data points can be weighted accordingly (e.g. small value of $T_i$ for the data point copies: the error is also made smaller).

\exercise

The following calculation assumes, that p is equal the the number of rows in the matrix \textbf{$\Phi$}. The normal least squares algorithm is defined as:
\begin{align}
	E_D(w) = \frac{1}{2} \sum^{N}[W^T\Phi(x_n)-z_n]^2= \frac{1}{2}(\textbf{$\Phi$}W-Z)^T(\textbf{$\Phi$}W-Z)
\end{align}
Augmenting the matrix and the vector by the given values leeds to:
\begin{align}
		\frac{1}{2} (
		\begin{pmatrix}
		\phi_0(X_1) & \dots & \phi_{m-1}(X_1) \\
		\vdots & \ddots & \vdots \\
		\phi_0(X_N) & \dots & \phi_{m-1}(X_N) \\
		\sqrt{\lambda} & \dots & 0 \\
		\vdots & \ddots & \vdots \\
		0 & \dots & \sqrt{\lambda}		
		\end{pmatrix}
		\begin{pmatrix}
		w_0 \\
		\vdots \\
		w_{m-1}
		\end{pmatrix}
		-
		\begin{pmatrix}
		z_0 \\
		\vdots \\
		z_{m-1} \\
		0 \\
		\vdots \\
		0
		\end{pmatrix} )^T
				\begin{pmatrix}
				\phi_0(X_1) & \dots & \phi_{m-1}(X_1) \\
				\vdots & \ddots & \vdots \\
				\phi_0(X_N) & \dots & \phi_{m-1}(X_N) \\
				\sqrt{\lambda} & \dots & 0 \\
				\vdots & \ddots & \vdots \\
				0 & \dots & \sqrt{\lambda}		
				\end{pmatrix}
				\begin{pmatrix}
				w_0 \\
				\vdots \\
				w_{m-1}
				\end{pmatrix}
				-
				\begin{pmatrix}
				z_0 \\
				\vdots \\
				z_{m-1} \\
				0 \\
				\vdots \\
				0
				\end{pmatrix} ) \\
				= \frac{1}{2}
				\begin{pmatrix}
				w_0\phi_0(X_1) - z_0 + \dots + w_{m-1}\phi_{m-1}(X_1) - z_{m-1} \\
				\vdots  \\
				w_0\phi_0{X_N} - z_0 + \dots + w_{m-1}\phi_{m-1}(X_N) - z_{m-1} \\
				\sqrt{\lambda}w_0 \\
				\vdots  \\	
				\sqrt{\lambda}w_{m-1}	
				\end{pmatrix}^T
				\begin{pmatrix}
				w_0\phi_0(X_1) - z_0 + \dots + w_{m-1}\phi_{m-1}(X_1) - z_{m-1} \\
				\vdots \\
				w_0\phi_0{X_N} - z_0 + \dots + w_{m-1}\phi_{m-1}(X_N) - z_{m-1} \\
				\sqrt{\lambda}w_0  \\
				\vdots  \\	
				\sqrt{\lambda}w_{m-1}	
				\end{pmatrix} \\
				= 
				\frac{1}{2}(\sum^{N}[W^T\Phi(x_n)]^2 + \lambda w_0^2 + \dots + \lambda w_{m-1}^2) = \frac{1}{2}(\sum^{N}[W^T\Phi(x_n)]^2 + \lambda ||w||) = 
				\frac{1}{2}\sum^{N}[W^T\Phi(x_n)]^2 + \frac{\lambda}{2} ||w|| = \widetilde{E}_D(w)
\end{align}

\exercise
\begin{align}
	p(W,\beta|Z,X) &\propto p(Z | X,W,\beta)p(W|\beta)p(\beta) \\
	&= p(Z | X,W,\beta)p(W,\beta) \\
	&= \prod_{n=1}^{N} \mathcal{N}(Z_n|W^T\phi(X_n),\beta^{-1}) \mathcal{N}(W|M_0,\beta^{-1}S_0)Gam(\beta|a_0,b_0)
\end{align}

\exercise
With $y \sim \mathcal{N}(10,4)$, then $p(y>10) = 0.5$. 10 is exactly the expected value of the normal distribution. To show this easily we can transform the random variable to a standard gaussian distribution.
\begin{align}
Z = \frac{y - \mu}{\sigma} = \frac{y - 10}{2}
\end{align}
To get the probability for $y>10$ we just have to plug in 10 into the formular for Z and look ap the corresponding value in the cummulative table. This results in $p(y>10) = 0.5$

\exercise
Given $y \sim \mathcal{N}(5x + 10,4)$ and $x=1$, then $y \sim \mathcal{N}(15,4)$.
Then the expected value of y is just 15.

\exercise
We just square the norm, which is ok because we are only interested in distances.
\begin{align}
	||x_1 - x_2||_2^2 &= <x_1 - x_2, x_1 - x_2> \\
	&= (x_1 - x_2)^T(x_1 - x_2) \\
	&= (x_1^T - x_2^T)(x_1 - x_2) \\
	&= x_1^Tx_1 - x_1^Tx_2 - x_2^Tx_1 + x_2^Tx_2 \\
	&= x_1^Tx_1 + x_2^Tx_2 - 2x_1^Tx_2
\end{align}
Using the kernel: $k(x_1,x_1) + k(x_2,x_2) - 2k(x_1,x_2)$

\exercise
\begin{align}
	k(x_1, x_2) &= exp\{-\frac{1}{2} ||x_1 - x_2||^2\} \\
	&= exp\{-\frac{1}{2} (x_1^Tx_1 + x_2^Tx_2 - 2x_1^Tx_2)\} \\
	&= exp\{-\frac{1}{2} x_1^Tx_1\}exp\{-\frac{1}{2} x_2^Tx_2\}exp\{x_1^Tx_2\}\\
	&= f(x_1)k(x_1, x_2)f(x_2)
\end{align}
with $f(x) = exp\{-\frac{1}{2}x^Tx\}$ and $k(x_1,x_2) = exp\{x_1^Tx_2 \}$.
$f(x)$ is only a scalar and a kernel times a scalar is also a kernel. $x_1^Tx_2$ is the linear kernel. Also $exp\{ k(x_1, x_2)\}$ is also a kernel.

\end{document}